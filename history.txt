在一大堆技术术语里，最为被普通人所熟知的大概就是“爬虫”了。其实爬虫这个名字就已经特别好地表现出了这项技术的作用——像密密麻麻的虫子一样分布在网络上，爬行至每一个角落获取数据；
几乎是和爬虫技术诞生的同一时刻，反爬虫技术也诞生了。在90年代开始有搜索引擎网站利用爬虫技术抓取网站时，一些搜索引擎从业者和网站站长通过邮件讨论定下了一项“君子协议”—— robots.txt。
即网站有权规定网站中哪些内容可以被爬虫抓取，哪些内容不可以被爬虫抓取。
这样既可以保护隐私和敏感信息，又可以被搜索引擎收录、增加流量。
爬虫技术刚刚诞生时我们还处于上古时代，互联网是一片贤者云集的乐土，大多数从业者都会默守这一协定，毕竟那时候信息和数据都没什么油水可捞。
但很快互联网上开始充斥着商品信息、机票价格、个人隐私……在利益的诱惑下，自然有些人会开始违法爬虫协议了。
当君子协议失效，我们开始改用技术手段阻拦爬虫的入侵。比如从访问数量上发现爬虫，当我们在某一网站浏览过快时，系统往往会要求输入验证码，就是因为这种快速浏览的行为很接近爬虫。
或者是不定期改变HTML标签，使之无法与Web排序匹配来限制爬虫。
但是即便如此，我们也没有任何方法可以禁止爬虫在网站中出入，只能加大爬虫的访问难度。如果网站可以供给人类访问，就一定也可以被爬虫访问。而且如果从底层完全组织爬虫抓取，也很可能让网站无法被搜索引擎收录。
当数据分析企业利用爬虫获取数据进行分析时，大量爬虫的存在正在让这些数据失实。文章浏览量的失实让我们误判人们对新闻事实的关注程度、爬虫衍生出的虚拟IP需要在数据清洗时剔除……技术越高超的爬虫，在行为模式上就越接近真人，也就更加增加数据分析时的难度。久而久之，那些我们以为从人类行为中寻找规律的算法，反而寻找到的是机器人的行为规律。
同时爬虫带来的流量波动也会让机器学习算法产生误判。
最典型的例子是机票的动态定价，网站会结合当下浏览量判定机票的抢手程度并且调整价格。这时如果有大量爬虫在浏览网站，算法就会给出和实际情况并不符合的定价，也损伤了消费者购买到廉价产品的权益。
甚至一些数据分析企业还打出了“AI爬虫”的招牌，让爬虫脚本的行为模式更加接普通用户，让被爬的企业难以发掘，甚至还会利用图像识别技术破解网站用作拦截的验证码。
在这种情况下，网站分辨人与机器人就变得更加困难也更加重要。很多网站也开始利用机器学习技术反制AI爬虫，比如为图形验证码动态打码应对图像识别。同时现在PC和移动终端的硬件技术发展，
也让生物识别这种更复杂的验证手段有可能加入战斗。双方正在站在同一水平线上，利用技术互相斗法。
